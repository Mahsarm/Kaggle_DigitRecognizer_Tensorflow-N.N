{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "#df.shape\n",
    "\n",
    "pixels_col = [col for col in df.columns if col != 'label']\n",
    "samples = df[pixels_col].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_digits(x):\n",
    "    res = np.zeros((10,), dtype=float)\n",
    "    res[x] = 1.0\n",
    "    return(res)\n",
    "target = np.apply_along_axis(one_hot_digits, axis=1, arr=df[['label']])\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33600, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, X_test, train_target, y_test = train_test_split(samples, target, test_size=0.2, random_state=4242)\n",
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "X = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "y = tf.placeholder(tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deep Neural Network Model:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>layers weigh shape :  (N_samples X N_features)(input of the layer) * (weights of the layer)(N_features X N_outputs) => (outputof the layer)((N_samples X N_outputs) + biases(N_outputs X 1)) </p>\n",
    "<p>biases shape: N_outputs X 1 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hl1_no_nodes = 500\n",
    "\n",
    "hl2_no_nodes = 500\n",
    "\n",
    "hl3_no_nodes = 500\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "hidden_layer_one = {'weights': tf.Variable(tf.random_normal(shape = [784, hl1_no_nodes])),\n",
    "                    'biases': tf.Variable(tf.random_normal( shape = [hl1_no_nodes]))}\n",
    "    \n",
    "hidden_layer_two = {'weights': tf.Variable(tf.random_normal(shape = (hl1_no_nodes, hl2_no_nodes))),\n",
    "                    'biases': tf.Variable(tf.random_normal( shape = [hl2_no_nodes]))}\n",
    "    \n",
    "hidden_layer_three = {'weights': tf.Variable(tf.random_normal(shape = [hl2_no_nodes, hl3_no_nodes])),\n",
    "                      'biases': tf.Variable(tf.random_normal( shape = [hl3_no_nodes]))}\n",
    "    \n",
    "output_layer = {'weights': tf.Variable(tf.random_normal(shape = [hl3_no_nodes, n_classes])),\n",
    "                'biases': tf.Variable(tf.random_normal( shape = [n_classes]))}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> (outputof the layer) = ((N_samples X N_outputs) + biases(N_outputs X 1))</p>\n",
    "<p>we apply rectifier leanier unit (relu) which is an activation function and is similar to sigomoid function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_output = tf.add(tf.matmul(X, hidden_layer_one['weights']) , hidden_layer_one['biases'])\n",
    "layer1_output = tf.nn.relu(layer1_output)\n",
    "\n",
    "layer2_output = tf.add(tf.matmul(layer1_output , hidden_layer_two['weights']) , hidden_layer_two['biases'])\n",
    "layer2_output = tf.nn.relu(layer2_output)\n",
    "\n",
    "layer3_output = tf.add(tf.matmul(layer2_output, hidden_layer_three['weights']) , hidden_layer_three['biases'])\n",
    "layer3_output = tf.nn.relu(layer3_output)\n",
    "\n",
    "output = tf.matmul(layer3_output, output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training the neural network:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>softmax is a logistic function () which predcits the probability of the classes (similar to sigmoid function)</p>\n",
    "<p>cross entropy: calcualtes the error or the difference between the predition and the target</p>\n",
    "<p>and reduce_mean (without specifiying the axis of cal) it returns calculates the mean of a tensor elements and  a single value</p>\n",
    "<p>and adamoptimizer minimizes the cost function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = output\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels = y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</p>we extecute opration objects by running session: </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>prediction or output is tensor of probablity, for finding the accryacy \n",
    "we can find the index of max value of probablity for each sample by using argmax and which gives\n",
    "us the digit and compare it to the target value of the sample:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> we use 100 cycle of feedforward to train the samples: epoch=100</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 1, 'completed out of', 100, 'loss:', 406771082.640625)\n",
      "('Epoch', 2, 'completed out of', 100, 'loss:', 86132229.869140625)\n",
      "('Epoch', 3, 'completed out of', 100, 'loss:', 46325805.282958984)\n",
      "('Epoch', 4, 'completed out of', 100, 'loss:', 27434520.31918335)\n",
      "('Epoch', 5, 'completed out of', 100, 'loss:', 16944088.495727539)\n",
      "('Epoch', 6, 'completed out of', 100, 'loss:', 10544153.481281281)\n",
      "('Epoch', 7, 'completed out of', 100, 'loss:', 7163444.1499710083)\n",
      "('Epoch', 8, 'completed out of', 100, 'loss:', 5013385.278137207)\n",
      "('Epoch', 9, 'completed out of', 100, 'loss:', 3415775.8520736694)\n",
      "('Epoch', 10, 'completed out of', 100, 'loss:', 2996060.2066385746)\n",
      "('Epoch', 11, 'completed out of', 100, 'loss:', 2707861.229552269)\n",
      "('Epoch', 12, 'completed out of', 100, 'loss:', 2546843.9262390137)\n",
      "('Epoch', 13, 'completed out of', 100, 'loss:', 2045772.6559867859)\n",
      "('Epoch', 14, 'completed out of', 100, 'loss:', 2025723.8417015076)\n",
      "('Epoch', 15, 'completed out of', 100, 'loss:', 2494893.0246210098)\n",
      "('Epoch', 16, 'completed out of', 100, 'loss:', 2032518.7678985596)\n",
      "('Epoch', 17, 'completed out of', 100, 'loss:', 2446145.2352676392)\n",
      "('Epoch', 18, 'completed out of', 100, 'loss:', 1525092.7333984375)\n",
      "('Epoch', 19, 'completed out of', 100, 'loss:', 1849191.8734226227)\n",
      "('Epoch', 20, 'completed out of', 100, 'loss:', 1951303.3290405273)\n",
      "('Epoch', 21, 'completed out of', 100, 'loss:', 1589636.6544451714)\n",
      "('Epoch', 22, 'completed out of', 100, 'loss:', 1537370.1527862549)\n",
      "('Epoch', 23, 'completed out of', 100, 'loss:', 1379970.2607421875)\n",
      "('Epoch', 24, 'completed out of', 100, 'loss:', 1596385.8193435669)\n",
      "('Epoch', 25, 'completed out of', 100, 'loss:', 1632169.6658630371)\n",
      "('Epoch', 26, 'completed out of', 100, 'loss:', 1355834.6098408885)\n",
      "('Epoch', 27, 'completed out of', 100, 'loss:', 1383359.1905517578)\n",
      "('Epoch', 28, 'completed out of', 100, 'loss:', 1232538.1379241943)\n",
      "('Epoch', 29, 'completed out of', 100, 'loss:', 1174555.1478729248)\n",
      "('Epoch', 30, 'completed out of', 100, 'loss:', 900688.353515625)\n",
      "('Epoch', 31, 'completed out of', 100, 'loss:', 1291266.4107818604)\n",
      "('Epoch', 32, 'completed out of', 100, 'loss:', 1181570.533203125)\n",
      "('Epoch', 33, 'completed out of', 100, 'loss:', 1301955.6862487793)\n",
      "('Epoch', 34, 'completed out of', 100, 'loss:', 1331656.5994262695)\n",
      "('Epoch', 35, 'completed out of', 100, 'loss:', 944503.13508605957)\n",
      "('Epoch', 36, 'completed out of', 100, 'loss:', 645486.81411743164)\n",
      "('Epoch', 37, 'completed out of', 100, 'loss:', 811562.52089691162)\n",
      "('Epoch', 38, 'completed out of', 100, 'loss:', 1348489.2622528076)\n",
      "('Epoch', 39, 'completed out of', 100, 'loss:', 927363.83493804932)\n",
      "('Epoch', 40, 'completed out of', 100, 'loss:', 1110782.7340377867)\n",
      "('Epoch', 41, 'completed out of', 100, 'loss:', 890923.55919647217)\n",
      "('Epoch', 42, 'completed out of', 100, 'loss:', 864161.29354858398)\n",
      "('Epoch', 43, 'completed out of', 100, 'loss:', 1236271.863494873)\n",
      "('Epoch', 44, 'completed out of', 100, 'loss:', 1007155.0653686523)\n",
      "('Epoch', 45, 'completed out of', 100, 'loss:', 860714.77944946289)\n",
      "('Epoch', 46, 'completed out of', 100, 'loss:', 730909.87309455872)\n",
      "('Epoch', 47, 'completed out of', 100, 'loss:', 504129.2864074707)\n",
      "('Epoch', 48, 'completed out of', 100, 'loss:', 958029.50675964355)\n",
      "('Epoch', 49, 'completed out of', 100, 'loss:', 869469.70454216003)\n",
      "('Epoch', 50, 'completed out of', 100, 'loss:', 592531.92672729492)\n",
      "('Epoch', 51, 'completed out of', 100, 'loss:', 762955.407289505)\n",
      "('Epoch', 52, 'completed out of', 100, 'loss:', 695169.75776863098)\n",
      "('Epoch', 53, 'completed out of', 100, 'loss:', 992582.03086471558)\n",
      "('Epoch', 54, 'completed out of', 100, 'loss:', 725379.33464050293)\n",
      "('Epoch', 55, 'completed out of', 100, 'loss:', 670458.11949157715)\n",
      "('Epoch', 56, 'completed out of', 100, 'loss:', 768810.65548706055)\n",
      "('Epoch', 57, 'completed out of', 100, 'loss:', 715241.95611572266)\n",
      "('Epoch', 58, 'completed out of', 100, 'loss:', 489577.01303100586)\n",
      "('Epoch', 59, 'completed out of', 100, 'loss:', 518742.4741153717)\n",
      "('Epoch', 60, 'completed out of', 100, 'loss:', 554387.87286376953)\n",
      "('Epoch', 61, 'completed out of', 100, 'loss:', 669592.85119628906)\n",
      "('Epoch', 62, 'completed out of', 100, 'loss:', 573953.55279541016)\n",
      "('Epoch', 63, 'completed out of', 100, 'loss:', 856129.65489196777)\n",
      "('Epoch', 64, 'completed out of', 100, 'loss:', 791106.2897644043)\n",
      "('Epoch', 65, 'completed out of', 100, 'loss:', 840324.34509277344)\n",
      "('Epoch', 66, 'completed out of', 100, 'loss:', 699632.92273330688)\n",
      "('Epoch', 67, 'completed out of', 100, 'loss:', 730687.32301330566)\n",
      "('Epoch', 68, 'completed out of', 100, 'loss:', 422497.97565460205)\n",
      "('Epoch', 69, 'completed out of', 100, 'loss:', 295059.11865615845)\n",
      "('Epoch', 70, 'completed out of', 100, 'loss:', 541519.34912109375)\n",
      "('Epoch', 71, 'completed out of', 100, 'loss:', 450763.86833953857)\n",
      "('Epoch', 72, 'completed out of', 100, 'loss:', 539156.1982421875)\n",
      "('Epoch', 73, 'completed out of', 100, 'loss:', 760062.97703552246)\n",
      "('Epoch', 74, 'completed out of', 100, 'loss:', 632955.52272701263)\n",
      "('Epoch', 75, 'completed out of', 100, 'loss:', 637778.71820068359)\n",
      "('Epoch', 76, 'completed out of', 100, 'loss:', 236695.72554016113)\n",
      "('Epoch', 77, 'completed out of', 100, 'loss:', 727555.31506347656)\n",
      "('Epoch', 78, 'completed out of', 100, 'loss:', 684100.28758239746)\n",
      "('Epoch', 79, 'completed out of', 100, 'loss:', 414414.9446105957)\n",
      "('Epoch', 80, 'completed out of', 100, 'loss:', 448380.81921386719)\n",
      "('Epoch', 81, 'completed out of', 100, 'loss:', 555504.45417785645)\n",
      "('Epoch', 82, 'completed out of', 100, 'loss:', 468998.00662612915)\n",
      "('Epoch', 83, 'completed out of', 100, 'loss:', 449544.28231811523)\n",
      "('Epoch', 84, 'completed out of', 100, 'loss:', 551183.06091308594)\n",
      "('Epoch', 85, 'completed out of', 100, 'loss:', 663064.25637817383)\n",
      "('Epoch', 86, 'completed out of', 100, 'loss:', 385578.70068174601)\n",
      "('Epoch', 87, 'completed out of', 100, 'loss:', 644699.04878997803)\n",
      "('Epoch', 88, 'completed out of', 100, 'loss:', 647736.48153686523)\n",
      "('Epoch', 89, 'completed out of', 100, 'loss:', 573786.72947311401)\n",
      "('Epoch', 90, 'completed out of', 100, 'loss:', 424872.01057434082)\n",
      "('Epoch', 91, 'completed out of', 100, 'loss:', 510445.0538482666)\n",
      "('Epoch', 92, 'completed out of', 100, 'loss:', 152014.92074584961)\n",
      "('Epoch', 93, 'completed out of', 100, 'loss:', 414577.44404506683)\n",
      "('Epoch', 94, 'completed out of', 100, 'loss:', 459382.10236358643)\n",
      "('Epoch', 95, 'completed out of', 100, 'loss:', 102262.12380981445)\n",
      "('Epoch', 96, 'completed out of', 100, 'loss:', 278932.22629547119)\n",
      "('Epoch', 97, 'completed out of', 100, 'loss:', 790676.23941040039)\n",
      "('Epoch', 98, 'completed out of', 100, 'loss:', 610262.78903198242)\n",
      "('Epoch', 99, 'completed out of', 100, 'loss:', 385350.42864990234)\n",
      "('Epoch', 100, 'completed out of', 100, 'loss:', 435426.66577148438)\n",
      "('Accuracy:', 96.297623)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "epochs = 100\n",
    "save_path = \"/home/mahsarm/Desktop/ML-projects/NN-digits/tf_3nn_model/\"\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        X_train, y_train = shuffle(train_samples, train_target, random_state=45)\n",
    "        for _ in range(train_samples.shape[0]/batch_size):\n",
    "           \n",
    "            train_feed_dict = {X: X_train[(_*batch_size):((_+1)*batch_size)],\n",
    "                         y:y_train[(_*batch_size):((_+1)*batch_size)]}\n",
    "            _, c = sess.run([optimizer, cost], train_feed_dict)\n",
    "            #print(c)\n",
    "            epoch_loss += c\n",
    "        print('Epoch', epoch, 'completed out of', epochs, 'loss:', epoch_loss)\n",
    "    #saving all the variables in the sess:\n",
    "    #saver.save(sess, save_path = save_path, global_step=0)\n",
    "    test_feed_dict = {X: X_test, y:y_test}\n",
    "    is_correct = tf.equal(tf.argmax(prediction,1) , tf.argmax(y,1))    \n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    print('Accuracy:', sess.run(accuracy*100, feed_dict=test_feed_dict))\n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
